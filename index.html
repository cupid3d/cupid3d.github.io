<!doctype html>
<html lang="en">

<head>
    <title>Cupid: Pose-Grounded Generative 3D Reconstruction from a Single Image</title>
    <link rel="icon" type="image/x-icon" href="/static/img/icons/cupid.png">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Open Graph -->
    <meta property="og:url" content="YOUR_PROJECT_URL_HERE" />
    <meta property="og:image" content="YOUR_PROJECT_URL_HERE/static/img/preview.png" />
    <meta property="og:title" content="Cupid: Pose-Grounded Generative 3D Reconstruction from a Single Image" />
    <meta property="og:description"
        content="A new generation-based 3D reconstruction method that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image." />

    <!-- Twitter -->
    <meta name="twitter:url" content="YOUR_PROJECT_URL_HERE" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:image" content="YOUR_PROJECT_URL_HERE/static/img/preview.png" />
    <meta name="twitter:title" content="Cupid: Pose-Grounded Generative 3D Reconstruction from a Single Image" />
    <meta name="twitter:description"
        content="A new generation-based 3D reconstruction method that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image." />

    <script src="./static/js/distill_template.v2.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    
    <!-- MathJax Configuration -->
    <script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams',
            macros: {
              argmin: '\\operatorname*{arg\\,min}',
              argmax: '\\operatorname*{arg\\,max}'
            }
          },
          options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://d3js.org/d3-collection.v1.min.js"></script>
    <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

    <script defer="" src="./static/js/hider.js"></script>
    <script src="./static/js/image_interact.js"></script>
    <script src="./static/js/switch_videos.js"></script>

    <link rel="stylesheet" href="./static/css/style.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script defer src="./static/js/fontawesome.all.min.js"></script>


    <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script> <!-- jquery -->
    <script defer src="./static/js/medium-zoom.min.js"></script>
    <script defer src="./static/js/zoom.js"></script>
</head>

<body>
    <div class="header-wrapper">
        <div class="header-container" id="header-container">
            <div class="header-content">
                <h1 style="margin-top: 0px"><i>CUPID</i></h1>
                <h2><i>Pose-Grounded</i>
                    Generative 3D Reconstruction from a Single Image</h2>
                <p>
                    Introducing Cupid, a 3D generator that accurately infers the
                    camera pose, 3D shape, and texture from a single image.
                </p>

                <div class="icon-container">
                    <div class="icon-item">
                        <img src="./static/img/icons/formula.svg" alt="Visual Representation Icon">
                        <div><strong>Novel Generative Formulation</strong>: We reframe 3D reconstruction by jointly
                            synthesizing the object and its camera pose. This grounds the generation process, creating
                            an explicit link between the 3D output and the 2D input view.
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/camera.svg" alt="Connector Design Icon">
                        <div><strong>Pose-Conditioned Architecture</strong>: We design a generator with a pose-aligned
                            conditioner. This novel mechanism directly injects <em>pixel information</em> into the
                            synthesis
                            process to prevent color drift and ensure textural fidelity.
                        </div>
                    </div>
                    <div class="icon-item">
                        <img src="./static/img/icons/cube.svg" alt="Instruction Tuning Data Icon">
                        <div><strong>Unified and Versatile Model</strong>: Our approach is a unified, versatile model
                            that
                            excels at diverse 3D synthesis tasks, including <em>single image reconstruction</em>, <em>scene
                                generation</em>, and <em>multi-view reconstruction</em>, without task-specific tuning.
                            </p>
                        </div>
                    </div>
                </div>

                <div class="button-container">
                    <!-- replace arxiv -->
                    <a href="https://arxiv.org/abs/2406.16860" class="button paper-link" target="_blank">
                        <span class="icon is-small">
                            <i class="ai ai-arxiv"></i>
                        </span>
                        arXiv
                    </a>
                    <!-- replace pdf -->
                    <a href="https://arxiv.org/pdf/2406.16860" class="button paper-link" target="_blank">
                        <span class="icon is-small">
                            <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>pdf</span>
                    </a>
                    <!-- replace image -->
                    <a href="https://github.com/cambrian-mllm/cambrian" class="button" target="_blank">
                        <span class="icon is-small">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                    </a>
                    <br>
                </div>
            </div>
            <div class="header-image">
                <img draggable="false" src="static/img/Cupid.png" alt="Teaser Image" class="teaser-image">
            </div>
        </div>
    </div>
    <d-article>
        <!-- for author information -->
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column" style="line-height: 1.2;">
                    <h3 style="font-size: 1.2em; margin-bottom: 0.2em;">Authors</h3>
                    <p style="margin: 0;">
                        <span class="author-link" style="font-size: 0.9em;">Binbin Huang</span>
                        <sup style="font-size: 0.9em;">1,2,†</sup>
                    </p>
                    <p style="margin: 0;">
                        <span class="author-link" style="font-size: 0.9em;">Haobin Duan</span>
                        <sup style="font-size: 0.9em;">1,†</sup>
                    </p>
                    <p style="margin: 0;">
                        <span class="author-link" style="font-size: 0.9em;">Yiqun Zhao</span>
                        <sup style="font-size: 0.9em;">1,2</sup>
                    </p>
                    <p style="margin: 0;">
                        <span class="author-link" style="font-size: 0.9em;">Zibo Zhao</span>
                        <sup style="font-size: 0.9em;">3</sup>
                    </p>
                    <p style="margin: 0;">
                        <a href="https://people.eecs.berkeley.edu/~yima/" class="author-link" target="_blank"
                            style="font-size: 0.9em;">Yi Ma</a>
                        <sup style="font-size: 0.9em;">1</sup>
                    </p>
                    <p style="margin: 0;">
                        <a href="https://scholar.google.com.sg/citations?user=fe-1v0MAAAAJ&hl=en" class="author-link"
                            target="_blank" style="font-size: 0.9em;">Shenghua Gao</a>
                        <sup style="font-size: 0.9em;">1,‡</sup>
                    </p>
                </div>

                <div class="byline-column" style="line-height: 1.2;">
                    <h3 style="font-size: 1.2em; margin-bottom: 0.2em;">Affiliations</h3>
                    <p style="margin: 0;">
                        <sup style="font-size: 0.8em;">1</sup>
                        <a href="https://www.hku.hk/" class="affiliation-link" target="_blank"
                            style="font-size: 0.9em;">HKU</a>
                    </p>
                    <p style="margin: 0;">
                        <sup style="font-size: 0.8em;">2</sup>
                        <span class="affiliation-link" style="font-size: 0.9em;">Transcengram</span>
                    </p>
                    <p style="margin: 0;">
                        <sup style="font-size: 0.8em;">3</sup>
                        <a href="https://www.tencent.com/" class="affiliation-link" target="_blank"
                            style="font-size: 0.9em;">Tencent</a>
                    </p>
                </div>

                <div class="byline-column" style="line-height: 1.2;">
                    <h3 style="font-size: 1.2em; margin-bottom: 0.2em;">Contact</h3>
                    <p style="margin: 0; font-size: 0.8em;">
                        binbinhuang@connect.hku.hk
                    </p>
                    <p style="margin: 0; font-size: 0.8em;">
                        haobinduan@connect.hku.hk
                    </p>
                    <p style="margin: 0; font-size: 0.8em;">
                        yiqun.zhao@connect.hku.hk
                    </p>
                    <p style="margin: 0; font-size: 0.8em;">
                        zibozhao@tencent.com
                    </p>
                    <p style="margin: 0; font-size: 0.8em;">
                        mayi@hku.hk
                    </p>
                    <p style="margin: 0; font-size: 0.8em;">
                        gaosh@hku.hk
                    </p>
                    <br>

                    <p style="text-align: left; margin: 0; font-size: 0.85em; color: #666;">
                        <span><sup>†</sup>Equal contribution</span><br>
                        <span><sup>‡</sup>Corresponding author</span><br>
                    </p>
                </div>
            </div>
        </div>

        <!-- Abstract Section -->
        <h1 class="text" style="text-align: center;">Abstract</h1>
        <p class="text abstract">
        This work proposes a new generation-based 3D reconstruction method, named <strong>Cupid</strong>, that
        accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image. Cupid casts 3D
        reconstruction as a conditional sampling process from a learned distribution of 3D objects, and it jointly
        generates voxels and pixel-voxel correspondences, enabling robust pose and shape estimation under a unified
        generative framework. By representing both input camera poses and 3D shape as a distribution in a shared 3D
        latent space, Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that produces initial 3D
        geometry with associated 2D projections for pose recovery; and (2) a refinement stage that integrates
        pose-aligned image features to enhance structural fidelity and appearance details. Extensive experiments
        demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3 dB PSNR gain and an over 10%
        Chamfer Distance reduction, while matching monocular estimators on pose accuracy and delivering superior visual
        fidelity over baseline 3D generative models.
        </p>

        <div class="icon-row">
            <a href="#visual-representation" class="icon-link">
                <img src="static/img/icons/visual.svg" alt="Visual Representation Logo" class="icon">
                Visual<br>Representations
            </a>
            <a href="#connector_design" class="icon-link">
                <img src="static/img/icons/connector.svg" alt="Connector Logo" class="icon">
                Connector<br>Design
            </a>
            <a href="#instruction_data" class="icon-link">
                <img src="static/img/icons/data.svg" alt="Data Logo" class="icon">
                Instruction<br>Data
            </a>
            <a href="#sec:inst_tuning" class="icon-link">
                <img src="static/img/icons/recipe.svg" alt="Recipe Logo" class="icon">
                Instruction<br>Recipes
            </a>
            <a href="#sec:benchmarking" class="icon-link">
                <img src="static/img/icons/eval.svg" alt="Eval Logo" class="icon">
                Evaluation<br>Protocol
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>

        <hr>

        <div id='method' class="vision-block">
            <div id="sec:method" class="sub-section">
                <h1 class="text">How Cupid Works?</h1>
                <!-- <p class="text">
                    Reconstructing a 3D object from a single 2D image is a classic challenge. A key difficulty is determining the camera's viewpoint. Our method, <strong>Cupid</strong>, introduces a novel solution: we don't just generate the 3D object, we <strong>jointly generate the object and its camera pose</strong> at the same time.
                </p> -->
                <!-- <h2 class="text">Method Overview</h2> -->
                <p class="text">
                    We use a powerful generative technique called Flow Model<d-cite
                        key="lipman2022flow"></d-cite>. This model learns to transform random noise into a voxelized 3D representation, guided by the input image. To make this process efficient and effective, we break it down into two main stages<d-cite
                        key="xiang2025structured"></d-cite>, as shown in <a href="#fig-method">Figure 1</a>.
                </p>

                <d-figure id="fig-method">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/method.png" alt="Overview of the Cupid method.">
                        <figcaption>
                            <strong>Figure 1: Overview of Cupid.</strong> Our two-stage process first generates a coarse shape and a novel "UV cube" to determine the camera pose. This pose then guides a second stage to generate high-fidelity geometry and appearance.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <p class="text">
                    <strong>Stage 1: Occupancy and Pose Generation. </strong>  
                    The first stage generates a coarse representation of the object and simultaneously estimates the camera pose. Given an input image, our flow model produces two key outputs: an occupancy cube (indicating which voxels $\mathbf{x}_i$ in space belong to the object) and a novel UV cube (indicating the 2D pixel locations $\mathbf{u}_i$ for each 3D voxel).
                    we can robustly solve for the camera's projection matrix $\mathbf{P}^{*}$ using a classical least-squares solver<d-cite
                        key="abdel2015direct"></d-cite>.
                </p>
                <p class="text" style="text-align: center;">
                    $$
                    \mathbf{P}^{*} = \argmin_{\mathbf{P}} \sum_{i} \big\Vert\pi(\mathbf{P},\mathbf{x}_i) - \mathbf{u}_i\big\Vert^2. \tag{1}\label{eq:pnp}
                    $$
                </p>

                <p class="text">
                    <strong>Stage 2: Pose-Aligned Geometry and Appearance Generation. </strong>
                    With the camera pose now known, the second stage generates the fine-grained geometry and appearance. A common problem here is "color drift" and "detail inconsistency", where the 3D model doesn't perfectly match the input image's colors and details. We solve this with a <strong>pose-aligned conditioner</strong> that inject pixel-wise information.
                </p>
                <p class="text">
                    For each voxel in the occupancy cube, we use the calculated pose to find exactly where it lands on the 2D input image. We then sample features (both high-level semantics from DINO and low-level color/texture) from that precise pixel location. These pixel-aligned features are injected directly into the generation process, ensuring the final 3D model has high-fidelity geometry and appearance that is faithful to the input view.
                </p>
                

                <h2 class="text">Extension 1: Composing Components for Full Scene Reconstruction</h2>
                <p class="text">
                    Our framework naturally extends to reconstructing entire scenes. We use an object detector (like SAM) to find all objects in an image. Then, we run our reconstruction process on each object independently.
                    Then, using the 3D-2D correspondences our method provides, we align each reconstructed object with a global depth prior (from a model like MoGe<d-cite
                        key="wang2025moge"></d-cite>). This allows us to solve for each object's correct scale and position, composing them into a single, coherent 3D scene.
                </p>
                
                <d-figure id="fig-scene">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/scene.png" alt="Component-aligned scene reconstruction.">
                        <figcaption>
                            <strong>Figure 2: Component-Aligned Scene reconstruction.</strong> By reconstructing each object and then solving for its similarity transformation, we can accurately compose complex 3D scenes.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <h2 class="text">Extension 2: Multi-view Reconstruction</h2>
                <p class="text">
                    Although <strong>Cupid</strong> is trained with single image condition, it can be easily extended to multi-view reconstruction, thanks to the flexibility of our generative framework.
                    Given multiple images of the same object from different angles, we know that the 3D object cube should be the same across all views.
                    Therefore, we can run our flow model for each image, but share the same occupancy latent $\mathbf{X}$ across all views during the iterative flow sampling. This is similar to MultiDiffusion<d-cite
                        key="bar2023multidiffusion"></d-cite> (which average the overlapped pixel regions in 2D during iterative sampling), but in the 3D space. 
                </p>

                <d-figure id="fig-multiview">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/multiview.png" alt="Multi-view conditioning results">
                        <figcaption>
                            <strong>Figure 3: Multi-view conditioning.</strong> When multiple input views are available, we fuse the shared object latent across flow paths (similar to MultiDiffusion<d-cite key="bar2023multidiffusion"></d-cite>), enabling camera, geometry and texture refinement across all images. <em>Top</em>: inputs; <em>Middle</em>: reconstructed 3D object and camera poses; <em>Bottom</em>: rendered images and geometry.
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
        </div>

        <div id='interactive_demo' class="interactive-demo-block">
            <div id="sec:interactive_demo" class="sub-section">
                <h1 class="text">Interactive 3D Visualization</h1>
                <p class="text">
                    Explore our reconstruction results interactively! The viewer below shows the full pipeline of <strong>Cupid</strong>: from the input image to the reconstructed 3D object with estimated camera pose.
                    <br><br>
                    <strong>Instructions:</strong> Use the navigation controls to rotate, pan, and zoom the 3D visualization. Click on the thumbnails below to switch between different reconstruction examples.
                </p>

                <div class="rerun-container">
                    <div id="viewer"></div>
                    
                    <div class="rerun-thumbnails">
                        <div class="rerun-thumbnail active" data-rrd="e7bf7ab5b5ae4eee9e126e59a74a61f1.rrd"
                            style="background: linear-gradient(135deg, #667eea, #764ba2)">
                            <span style="font-size: 2em;">🎨</span>
                            <span style="font-size: 0.8em; margin-top: 0.5em;">Example 1</span>
                        </div>
                        <div class="rerun-thumbnail" data-rrd="9dc95735ed6c4b83971ed486a0fa5acb.rrd"
                            style="background: linear-gradient(135deg, #f093fb, #f5576c)">
                            <span style="font-size: 2em;">🔬</span>
                            <span style="font-size: 0.8em; margin-top: 0.5em;">Example 2</span>
                        </div>
                    </div>
                    
                    <!-- <div id="status" class="rerun-loading">Loading...</div> -->
                </div>
            </div>

            <script type="module">
                import { initRerunViewer } from './static/js/rerun-viewer.js';

                // Initialize the Rerun viewer with recording-blueprint pairs
                initRerunViewer({
                    containerId: 'viewer',
                    statusId: 'status',
                    baseUrl: 'http://localhost:8000/',
                    recordings: [
                        {
                            rrd: 'e7bf7ab5b5ae4eee9e126e59a74a61f1.rrd',
                            // blueprint: 'cupid_visualization_1.rbl'
                        },
                        {
                            rrd: '9dc95735ed6c4b83971ed486a0fa5acb.rrd',
                            // blueprint: 'cupid_visualization_2.rbl'
                        }
                    ],
                    loadOnClick: false  // Preload mode: load all recordings at startup
                });
            </script>
        </div>
    </d-article>



    <d-appendix>
        <h3>BibTeX</h3>
        <p class="bibtex">
            TBD
        </p>

        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
    </d-appendix>
    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <script src="./static/js/nav-bar.js"></script>
</body>
</html>